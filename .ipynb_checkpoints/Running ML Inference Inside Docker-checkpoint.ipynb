{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "**Programmer: python_scripts (Abhijith Warrier)**\n",
    "\n",
    "**PYTHON SCRIPT TO *RUN MACHINE LEARNING INFERENCE INSIDE A DOCKER CONTAINER USING A FASTAPI API*. üß†üê≥‚ö°**\n",
    "\n",
    "This script demonstrates how a trained machine learning model performs inference **inside a running Docker container**. We focus on verifying that predictions work exactly the same way inside Docker as they do locally‚Äîconfirming a successful containerized deployment."
   ],
   "id": "8e7f475215a95ffb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "1db316088379786b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üì¶ Install Required Packages**\n",
    "\n",
    "Install the necessary libraries to run the ML inference API locally before containerizing it."
   ],
   "id": "28725106897c1775"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install fastapi uvicorn scikit-learn joblib numpy",
   "id": "f4214ae1a80f682d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "5de3c8df9afcda13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üß© Load the Trained Model Inside the API**\n",
    "\n",
    "Load the saved ML model at application startup so it‚Äôs reused for every inference call."
   ],
   "id": "2c603e9e6d0efa2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "model = joblib.load(\"iris_model.joblib\")\n",
    "\n",
    "app = FastAPI(title=\"ML Inference Inside Docker\")\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    features: list"
   ],
   "id": "e5ddaa52bfe7dc29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "a5b0c1cdc04c392c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üîÆ Define the Inference Endpoint**\n",
    "\n",
    "Accept input features, reshape them correctly, and return the model‚Äôs prediction."
   ],
   "id": "dd2b6ec792bbbd6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@app.post(\"/predict\")\n",
    "def predict(data: InputData):\n",
    "    arr = np.array(data.features).reshape(1, -1)\n",
    "    prediction = model.predict(arr).tolist()\n",
    "    return {\"prediction\": prediction}"
   ],
   "id": "b72f0ae3ddfd96a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "b8ea7e83062817c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üê≥ Run the Inference API Inside Docker**\n",
    "\n",
    "Start the Docker container that hosts the ML inference service."
   ],
   "id": "6f3228c23a419573"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docker run -p 8000:8000 iris-ml-api",
   "id": "d5172936a8201307"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This runs the FastAPI app **entirely inside Docker**, exposing it on port 8000.",
   "id": "42db65389e898433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "cd145c39c505bf87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üåê Send Inference Requests to the Running Container**\n",
    "\n",
    "Call the prediction endpoint to verify inference works inside the container."
   ],
   "id": "9a695aeffdeaadb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "curl -X POST \"http://127.0.0.1:8000/predict\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"features\":[5.8,2.7,5.1,1.9]}'"
   ],
   "id": "e6604db8918359d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The response confirms successful ML inference inside Docker.",
   "id": "fd1206eb9e5a115a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "42c562432093cff3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üß™ Why This Step Matters**\n",
    "\n",
    "**This step validates that:**\n",
    "\n",
    "- the model loads correctly inside Docker\n",
    "- dependencies are properly installed\n",
    "- inference logic behaves identically across environments\n",
    "- the container is ready for cloud deployment"
   ],
   "id": "5973efa204c6a466"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "28673ffed49c53dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ **Key Takeaways**\n",
    "\n",
    "1. ML inference can run reliably inside Docker containers.\n",
    "2. Loading the model at startup improves inference performance.\n",
    "3. Docker ensures consistent predictions across machines.\n",
    "4. Containerized inference is a prerequisite for cloud deployment.\n",
    "5. This confirms your ML app is production-ready at a basic level."
   ],
   "id": "df07910973e72345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "8b83614af974d961"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1fa57bbde7af5207"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
