{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "**Programmer:** python_scripts (Abhijith Warrier)\n",
    "\n",
    "**PYTHON SCRIPT TO _EXPLAIN MACHINE LEARNING MODEL PREDICTIONS USING SHAP VALUES_. üß†üìäüîç**\n",
    "\n",
    "This script demonstrates how to use **SHAP (SHapley Additive exPlanations)** to interpret machine learning models by explaining **how each feature contributes** to a prediction ‚Äî both **globally** and **locally**."
   ],
   "id": "299a9d8716b16ae4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "9366cc8a7c648118"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **üì¶ Install Required Packages**",
   "id": "7f924d3342b8056a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install shap scikit-learn pandas matplotlib",
   "id": "2a0bb27f26d8d5ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "6b74e06f89cc1150"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üß© Load Dataset and Train a Model**\n",
    "\n",
    "We‚Äôll use a simple tabular dataset and a tree-based model (ideal for SHAP)."
   ],
   "id": "a248075c89b20ab6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "e3416cf850fa1b52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "f23a1702a899119e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üîé Initialise SHAP Explainer**\n",
    "\n",
    "SHAP needs a model-specific explainer to compute feature contributions."
   ],
   "id": "89b5fce913e9bc42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shap\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)"
   ],
   "id": "d08e3127cf8b92a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "e48d53c1dc4e7185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üìä Global Feature Importance (SHAP Summary Plot)**\n",
    "\n",
    "This shows **which features matter most overall** and **how they affect predictions**."
   ],
   "id": "30889b47e7d3e664"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shap.summary_plot(\n",
    "    shap_values[1],\n",
    "    X_test\n",
    ")"
   ],
   "id": "ba26164a9f4af081"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Features are ranked by importance\n",
    "- Color shows feature value (low ‚Üí high)\n",
    "- Direction shows positive or negative impact"
   ],
   "id": "3501dfb0bd95182b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "53e39c8938990be8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üîç Local Explanation (Single Prediction)**\n",
    "\n",
    "Explain *why* the model made a specific prediction."
   ],
   "id": "80e5e2f03a9225c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_index = 0\n",
    "\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[1][sample_index],\n",
    "    X_test.iloc[sample_index],\n",
    "    matplotlib=True\n",
    ")"
   ],
   "id": "c2b5b177ea6b5e68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This visualises how each feature pushes the prediction higher or lower.",
   "id": "1776f9cbf86d3002"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "a3d3511ac81ff8cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üìà SHAP Dependence Plot (Feature Interaction)**\n",
    "\n",
    "Understand how a feature influences predictions across values."
   ],
   "id": "9c8968dd9da09147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shap.dependence_plot(\n",
    "    \"mean radius\",\n",
    "    shap_values[1],\n",
    "    X_test\n",
    ")"
   ],
   "id": "69e2976ce65f8de6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This reveals:\n",
    "\n",
    "- non-linear effects\n",
    "- interactions with other features"
   ],
   "id": "7a83240a1f5d6e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "e60434ae70044070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **üß† Why SHAP Matters**\n",
    "\n",
    "- Explains complex ML models transparently\n",
    "- Works with tree, linear, and deep models\n",
    "- Essential for trust, debugging, and compliance\n",
    "- Widely used in production ML systems"
   ],
   "id": "367142d96cd9218e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "1326766e9dc80a3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Key Takeaways**\n",
    "\n",
    "1. SHAP explains *why* a model makes predictions.\n",
    "2. It provides both global and local interpretability.\n",
    "3. Tree-based models integrate seamlessly with SHAP.\n",
    "4. Visual explanations improve trust and debugging.\n",
    "5. Interpretability is essential for real-world ML, not optional."
   ],
   "id": "424f432d41077681"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "9452ff4124678fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Conclusion**\n",
    "\n",
    "SHAP bridges the gap between **model performance** and **human understanding**.\n",
    "\n",
    "By visualising feature contributions, we gain transparency into otherwise opaque machine learning models.\n",
    "\n",
    "This project demonstrates how **advanced visualisation and interpretability** elevate ML systems from experimental models to **production-ready solutions**."
   ],
   "id": "cd01e222e2fcd01f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "67aab839e26bfd7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6027b59bbbf83d3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
