{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Programmer:** python_scripts (Abhijith Warrier)\n",
    "\n",
    "**PYTHON SCRIPT TO *PREDICT TITANIC SURVIVAL WITH IMPROVED FEATURE ENGINEERING*. ðŸš¢ðŸ¤–ðŸ“Š**\n",
    "\n",
    "This script demonstrates how to build a **Titanic Survival Prediction Model** using advanced **feature engineering** and **machine learning pipelines**.\n",
    "\n",
    "It extracts insightful features like **Title, Family Size, Cabin Deck, Fare per Person, and Ticket Group Size**, performs **targeted imputation** for missing ages, and uses **Gradient Boosting** with cross-validation for robust performance â€” achieving smarter survival predictions with interpretable engineered features."
   ],
   "id": "fe5be2982ec51082"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "affdd03dc52e6005"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ðŸ“¦ Import Required Libraries**",
   "id": "2e925d54abcea185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "f405baf5b9a1d32c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "f5a0143c597964b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **ðŸ§© Load the Dataset**\n",
    "\n",
    "Use your **Kaggle Titanic** If you donâ€™t have the file handy, this cell shows the expected columns."
   ],
   "id": "32495a6ffb23f657"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Path to Kaggle Titanic train file (adjust as needed)\n",
    "CSV_PATH = \"datasets/titanic.csv\"  # e.g., \"/path/to/train.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head(3)"
   ],
   "id": "9806ca683658129b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Expected columns:** PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked",
   "id": "264299930228b805"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "bf5aebd3495de7be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **ðŸ› ï¸ Feature Engineering (High-Impact Additions)**\n",
    "\n",
    "Weâ€™ll create:\n",
    "\n",
    "- **Title** from Name â†’ (Mr, Mrs, Miss, Master, Rare)\n",
    "- **FamilySize** = SibSp + Parch + 1\n",
    "- **IsAlone** (1 if FamilySize==1)\n",
    "- **TicketGroupSize** (count of same ticket)\n",
    "- **CabinDeck** (first letter of Cabin, e.g., A/B/C; U if unknown)\n",
    "- **FarePerPerson** = Fare / FamilySize\n",
    "- **LogFare** and **AgeBin** (quantile bins)\n",
    "- **Targeted Age impute** using grouped median by (Title, Sex, Pclass) as a preprocessing step"
   ],
   "id": "b4fad04ef18b51df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_title(name: str) -> str:\n",
    "    # Titles appear as \"Lastname, Title. Firstname\"\n",
    "    if pd.isna(name):\n",
    "        return \"Unknown\"\n",
    "    title = name.split(\",\")[1].split(\".\")[0].strip()\n",
    "    # Map rare titles\n",
    "    mapping = {\n",
    "        \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\",\n",
    "        \"Lady\": \"Rare\", \"Countess\": \"Rare\", \"Sir\": \"Rare\",\n",
    "        \"Jonkheer\": \"Rare\", \"Don\": \"Rare\", \"Dona\": \"Rare\",\n",
    "        \"Capt\": \"Rare\", \"Col\": \"Rare\", \"Major\": \"Rare\",\n",
    "        \"Rev\": \"Rare\", \"Dr\": \"Rare\"\n",
    "    }\n",
    "    return mapping.get(title, title)\n",
    "\n",
    "def cabin_deck(cabin: str) -> str:\n",
    "    if pd.isna(cabin) or not str(cabin).strip():\n",
    "        return \"U\"  # Unknown\n",
    "    return str(cabin)[0]\n",
    "\n",
    "def engineer_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Title\n",
    "    df[\"Title\"] = df[\"Name\"].apply(extract_title)\n",
    "    # Simplify extremely rare titles\n",
    "    freq = df[\"Title\"].value_counts()\n",
    "    rare_titles = freq[freq < 10].index\n",
    "    df[\"Title\"] = df[\"Title\"].replace(dict.fromkeys(rare_titles, \"Rare\"))\n",
    "\n",
    "    # Family size & solitude\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
    "\n",
    "    # Ticket group size\n",
    "    df[\"TicketGroupSize\"] = df.groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")\n",
    "\n",
    "    # Cabin deck\n",
    "    df[\"CabinDeck\"] = df[\"Cabin\"].apply(cabin_deck)\n",
    "\n",
    "    # Fare per person & log transforms (protect against zero)\n",
    "    df[\"FarePerPerson\"] = df[\"Fare\"] / df[\"FamilySize\"].replace(0, 1)\n",
    "    df[\"LogFare\"] = np.log1p(df[\"Fare\"])\n",
    "    df[\"LogFarePerPerson\"] = np.log1p(df[\"FarePerPerson\"])\n",
    "\n",
    "    # Age bin (quantiles on available values)\n",
    "    df[\"AgeBin\"] = pd.qcut(df[\"Age\"], q=4, duplicates=\"drop\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_eng = engineer_features(df)\n",
    "df_eng.head(3)"
   ],
   "id": "240488b632bb42b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "ce1e855a33fad891"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **ðŸ§¼ Age Imputation by Group (Title Ã— Sex Ã— Pclass)**\n",
    "\n",
    "Weâ€™ll fill **Age** using the median within (Title, Sex, Pclass) groups.\n",
    "\n",
    "Then weâ€™ll recompute **AgeBin** after imputation."
   ],
   "id": "72e5d5e508cbc1e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_tmp = df_eng.copy()\n",
    "\n",
    "age_group_median = (\n",
    "    df_tmp.groupby([\"Title\", \"Sex\", \"Pclass\"])[\"Age\"]\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Age\": \"AgeMedian\"})\n",
    ")\n",
    "\n",
    "df_tmp = df_tmp.merge(age_group_median, on=[\"Title\", \"Sex\", \"Pclass\"], how=\"left\")\n",
    "df_tmp[\"Age\"] = df_tmp[\"Age\"].fillna(df_tmp[\"AgeMedian\"])\n",
    "df_tmp.drop(columns=[\"AgeMedian\"], inplace=True)\n",
    "\n",
    "# Recompute AgeBin with filled Age\n",
    "df_tmp[\"AgeBin\"] = pd.qcut(df_tmp[\"Age\"], q=4, duplicates=\"drop\")\n",
    "\n",
    "df_eng = df_tmp\n",
    "df_eng.isna().sum()"
   ],
   "id": "53c9612d91e2f5a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "5134a3b3f0eaf0a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ðŸ§¾ Select Features & Train/Test Split**",
   "id": "2e3e229fd555b93e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TARGET = \"Survived\"\n",
    "\n",
    "features = [\n",
    "    # Numeric\n",
    "    \"Age\", \"Fare\", \"FamilySize\", \"IsAlone\", \"TicketGroupSize\",\n",
    "    \"FarePerPerson\", \"LogFare\", \"LogFarePerPerson\",\n",
    "    # Categorical\n",
    "    \"Pclass\", \"Sex\", \"Embarked\", \"Title\", \"CabinDeck\", \"AgeBin\"\n",
    "]\n",
    "\n",
    "X = df_eng[features]\n",
    "y = df_eng[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ],
   "id": "55aca90c60d8accb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "f8d0ebecd69a4201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **ðŸ—ï¸ Preprocessing Pipeline**\n",
    "\n",
    "- **Numeric** â†’ impute median â†’ scale\n",
    "- **Categorical** â†’ impute most frequent â†’ one-hot"
   ],
   "id": "f6ceaa1d51f6f189"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_features = [\"Age\", \"Fare\", \"FamilySize\", \"IsAlone\", \"TicketGroupSize\", \"FarePerPerson\", \"LogFare\", \"LogFarePerPerson\"]\n",
    "categorical_features = [\"Pclass\", \"Sex\", \"Embarked\", \"Title\", \"CabinDeck\", \"AgeBin\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ],
   "id": "2361e10a179e420"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "c851d4b0d63a3bdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **ðŸ¤– Train Strong Baselines (LogReg / GBDT / RF)**\n",
    "\n",
    "Weâ€™ll evaluate **ROC AUC** with stratified CV and then fit final models."
   ],
   "id": "1f0bcf94cb934fdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=5000, C=1.0, class_weight=None, n_jobs=None if hasattr(LogisticRegression, \"n_jobs\") else None),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"clf\", clf)])\n",
    "    scores = cross_val_score(pipe, X_train, y_train, scoring=\"roc_auc\", cv=cv)\n",
    "    cv_scores[name] = (scores.mean(), scores.std())\n",
    "    print(f\"{name}: ROC AUC {scores.mean():.3f} Â± {scores.std():.3f}\")\n",
    "\n",
    "# Pick one strong model for final fit (GBDT is often great here)\n",
    "final_model = Pipeline(steps=[(\"prep\", preprocessor), (\"clf\", GradientBoostingClassifier(random_state=42))])\n",
    "final_model.fit(X_train, y_train)"
   ],
   "id": "82bec7f1e56f3ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "2871cd7a4b71c54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ðŸ“Š Evaluate on Test Set**",
   "id": "d007c372ae3d523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, y_proba).round(3))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Survived\", \"Survived\"])\n",
    "disp.plot(cmap=\"Purples\")\n",
    "plt.title(\"Titanic â€“ Confusion Matrix (Test)\")\n",
    "plt.show()"
   ],
   "id": "d9e30f8544d3f4a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "551b0ff82eca0879"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ðŸ”Ž Quick Hyperparameter Tune (GBDT)**",
   "id": "df0f92fa7f0d8996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "    \"clf__n_estimators\": [100, 200, 300],\n",
    "    \"clf__learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"clf__max_depth\": [2, 3],\n",
    "    \"clf__subsample\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gbdt = Pipeline(steps=[(\"prep\", preprocessor), (\"clf\", GradientBoostingClassifier(random_state=42))])\n",
    "grid = GridSearchCV(gbdt, param_grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best AUC:\", grid.best_score_)\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "print(\"Test AUC (best):\", roc_auc_score(y_test, y_proba_best).round(3))"
   ],
   "id": "fb009d0a83599a41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "e00d8592d43880e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **ðŸ“¤ Predict for Submission (Optional Kaggle Step)**",
   "id": "697c66e744ca6168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# If you want to create a Kaggle submission:\n",
    "# Load test.csv, apply the SAME feature engineering, then predict.\n",
    "\n",
    "TEST_CSV_PATH = \"test.csv\"  # adjust if needed\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "test_eng = engineer_features(test_df)\n",
    "\n",
    "# Impute Age via the same grouped median strategy (fit on TRAIN!)\n",
    "age_group_median_train = (\n",
    "    df_eng.groupby([\"Title\", \"Sex\", \"Pclass\"])[\"Age\"]\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Age\": \"AgeMedian\"})\n",
    ")\n",
    "\n",
    "test_eng = test_eng.merge(age_group_median_train, on=[\"Title\", \"Sex\", \"Pclass\"], how=\"left\")\n",
    "test_eng[\"Age\"] = test_eng[\"Age\"].fillna(test_eng[\"AgeMedian\"])\n",
    "test_eng.drop(columns=[\"AgeMedian\"], inplace=True)\n",
    "test_eng[\"AgeBin\"] = pd.qcut(test_eng[\"Age\"], q=4, duplicates=\"drop\")\n",
    "\n",
    "X_submit = test_eng[features]\n",
    "pred_submit = best_model.predict(X_submit) if 'best_model' in globals() else final_model.predict(X_submit)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_df[\"PassengerId\"],\n",
    "    \"Survived\": pred_submit.astype(int)\n",
    "})\n",
    "submission.head()\n",
    "# submission.to_csv(\"submission.csv\", index=False)"
   ],
   "id": "e29ea6ce3319e545"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "cddcf4405e4985e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7ed55c52511296a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
